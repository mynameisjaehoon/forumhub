---
title: 애플리케이션 성능 테스트로 서버 튜닝하기
date: 2024-04-19 18:00:00 +09:00
description: >-
    웹 서버 성능 테스트를 시행하고 튜닝과정에 대해서 설명합니다.
categories: [성능 테스트]
tags: [메이플 주문서 시뮬레이터]
---

## 성능테스트 이유
내가 운영하고 있는 서비스는 일정한 수의 사용자가 꾸준히 사용하는 서비스가 아니다. 게임과 관련된 서비스이기 때문에 게임이 업데이트 되거나 어떤 유튜버가 서비스를 사용하고 올린 영상에 사이트가 노출되었을 때 조회요청이 올라가는 경향이 있다.
![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/34f721cd-2e27-4a88-ab83-5d1658702dd9)
위 차트는 사용자 수를 나타내는 차트이다. 한눈에 봐도 높은 구간이 있는데, 270~80여명 정도의 사용자가 동시에 접속했고 저때 일순간이였지만 응답시간이 굉장히 느려졌다. 피크를 지난 다음에는 조금 완화되었지만 그래도 100여명 정도의 사용자가 동시접속하는 상황이 지속되면서 응답시간이 느려지는 문제가 있었다.

성능테스트를 통해서 최소한 저정도의 트래픽이 들어왔을 때 서버가 다운되지 않고 목표한 만큼의 응답시간을 내고싶었고 그러기 위해서는 웹 애플리케이션 스레드 수를 조절하거나, DB 의 커넥션 수를 조절하거나, 쿼리를 튜닝하는 등의 작업이 필수적이다. 그리고 그 원인을 찾기 위해서 성능테스트를 진행하였다.

성능을 올리는 방법으로는 스레드, 커넥션 수 조절말고도 WAS를 늘리는 Scale-out을 사용할 수 있다. 하지만 또 서버를 늘리기 비용이 부담되어 우선은 Scale-Out은 제쳐두기로 하였다. 여유가 되면 당연히 고려할것이다.

## 성능테스트 도구
- 성능 테스트 도구에는 `nGrinder`
- 모니터링에는 `Grafana`

응답 시간과 TPS, 그리고 CPU, 메모리 사용량 뿐만 아니라 애플리케이션 내부의 병목구간을 더 자세히 알아보기 위해서 `PinPoint`를 사용하는 것도 고려하였으나 JDK 버전 문제, 로컬 컴퓨터의 공유기 방화벽으로 에이전트와 서버가 원활히 통신하지 못하는 문제에 부딪혀 `Grafana`로 모니터링하기로 했다. 이번 성능테스트의 목적이 애플리케이션 내부의 병목을 찾거나, 데이터베이스 쿼리를 수정하기위해서가 아니라 애플리케이션 스레드 풀의 스레드 수나 DB 커넥션 수를 조절하기 위해서 이기 때문에 `PinPoint`에서 제공하는 디테일한 정보까지는 필요하지 않았다.

PinPoint는 애플리케이션의 구성과 각 요소들의 관계를 파악할 수 있도록 보여주고 문제발생 지점과 병목구간을 쉽게 발견하여 문제진단을 더 빠르게할 수 있도록 도와준다.

## 테스트 계획
### 튜닝 목표

#### 목표 RPS 계산
두달 동안 서비스를 운영하면서 가장 높았던 DAU 2,400 이다. 하지만 계속해서 서비스가 성장해나가고 업데이트도 이루어지고 있는 추세이기 때문에 욕심을 내서 DAU 10,000을 목표로 잡고 튜닝을 시도해보자. 

그리고 서비스의 특수성을 고려해야한다. 내가 운영하고 있는 서비스는 시뮬레이션이라는 카테고리 특성상 한번 들어온 사용자가 꾸준하게 오랜시간 사용하거나 주기적으로 들어와서 이용하는 경우는 드물다. 게임에 업데이트가 되거나, 특정 유튜버가 우연히 서비스를 사용했을 때 사용자의 수가 급격하게 늘어나는 특징을 가지고 있다. DAU 10,000을 목표로 잡았기 때문에 시간당 평균 접속자 수는 `10,000 ÷ 24 = 416` 이지만 한시간 동안 고르게 사용자가 접속하는 것이 아니라 짧은 시간동안 한번에 몰아서 들어오기 때문에 알려져 있는 튜닝 목표 계산식 보다는 기준을 높게 잡는 것이 좋다.

> - Throughput: 1일 평균 RPS ~ 1일 최대 RPS
> - (1일 사용자 수(DAU)) * (1명당 1일 평균 접속 수) = 1일 총 접속 수
> - (1일 총 접속 수) / 86,400 = 1일 평균 RPS
> - (1일 평균 RPS) * (최대 트래픽 / 평소 트래픽) = 1일 최대 RPS

위 공식으로 계산했을 때 수용해야하는 Vuser는 25.02로 약 25명정도의 Vuser를 감당할 수 있는 정도면 된다. 그러나 나느 최악의 상황에서도 대비하고 싶고, 실제 눈으로 한번에 200명 이상의 사용자가 접속하는것을 눈으로 목격했기 때문에 공식으로 나온 결과보다 목표치를 상향해서 설정했다. 따라서 Vuser가 최소한 50 정도일 때 100ms의 응답속도를 목표로 하고있다.

**`1일 총 요청 수`**<br>
= DAU x (1명당 1일 평균 요청 수)<br>
= 1만 x 5(1명당 5개의 아이템 시뮬레이션을 사용한다 가정)<br>
= 5만

**`초당 평균 요청 수(Request Per Second)`**<br>
= 1일 총 요청 수 ÷ 하루 24시간을 초로 환산<br>
= 5만 ÷ 86400 ≈ 0.57<br>

하지만 위에서 설명했듯이 사용자가 짧은 시간동안 급격하게 늘어나는 특징을 가지고 있는 서비스이다. 
그리고 지금까지 서비스를 운영하면서 측정해온 초당 요청 수의 최댓값이 `60`이기 때문에 그 2배인 `120`을 목표로 두고자한다.

#### 최종 목표
1. 사용자가 1초에 120번의 요청을 하는 상황에서 각 요청이 150ms 안에 제대로 된 응답을 돌려준다.
2. 문제가 없다면 서버의 최대 성능을 알아보기 위해 트래픽을 넘어서는 부하를 주어 문제가 발생하는 지점을 찾는다.


### 테스트 도구
성능 테스트 도구에는 K6, nGrinder, JMeter등의 도구가 있습니다. 

### 테스트 환경
#### 테스트 대상 서버
- GCP VM `e2-small` 인스턴스
    - Intel Broadwell 2코어 `2.25GHz`
    - 2GB RAM
#### 에이전트 위치 
서버가 위치한 망과 같은 망에 에이전트가 위치하게 된다면 네트워크에 부하가 발생했을 때 실제 상황과는 괴리가 있는 결과가 발생할 수 있습니다. 
따라서 에이전트는 애플리케이션 서버가 아닌 다른 서버 로컬에 위치시켰습니다.

### 테스트 스크립트 작성
nGrinder 스크립트를 작성하기 위해서는 다음과 같은 요소들이 필요합니다.
1. 부하를 줄 가상 사용자 수(Vuser)
2. 부하 시간

#### 테스트 실행
일반적으로 성능테스트를 진행할 때는 30분에서 2시간정도 긴 시간을 소요하는 내구성 테스트를 진행한다고 한다. 하지만 내 서비스의 경우에는 10분이상 사용하는 경우가 드물다.
따라서 사용자의 특성을 고려해서 최소 트래픽에서 최대 트래픽까지 10분간 천천히 올리고 5분간 최대 트래픽 상황에서 테스트를 진행했다.

아래의 테스트 결과는 두번 테스트를 진행한 결과이다. 비슷한 경향을 보이는 것을 볼 수 있다.

![두번째 테스트 결과](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/f3f2a457-21ed-47b6-8da9-6ffcf578f5b2)
![데이터베이스 커넥션 결과](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/d888195c-1ff0-4e3a-92f0-6ed4e63056e6)

#### 테스트 결과 분석하기

부하테스트 시 TPS 그래프는 각 영역이 나누어진다.

![부하테스트 TPS 그래프](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/5586a659-11db-4d6b-8a88-e7d1fc485278)

- 저부하 구간(Light Load Zone): 사용자가 증가할수록 처리량도 같이 증가하는 구간입니다.
- 포화점/임계점(Saturation Point): 사용자가 증가해도 처리량이 더 이상 증가하지 않는 구간입니다. 시스템의 리소스 부족이나 병목발생으로 인해 나타나는 현상입니다.
- 경합구간(Buckle Zone): 포화점이 지속되어 2차병목이 발생하고 처리량이 오히려 감소하는 구간이다.

테스트 결과를 여러 구간들과 관련지어 분석해보면
- 두 테스트 모두 Vuser가 10쯤 되는 시점에서 포화점이 시작되고 25명쯤일 때 경합구간에 진입한다.
- 경합구간에 들어간 후 응답시간도 160ms 밑으로 내려오지 않아 목표인 150ms에 미치지 못한다.
- 경합구간에 들어간 후 CPU사용량은 최대 70%로 꽤있는 편이지만 아직 사용할 수 있는 리소스가 남아있는 상태.
- WAS의 스레드 풀에서 사용할 수 있는 스레드의 수도 여유가 있음.

## 문제 해결 과정

### USE 방법론
문제를 정의했으니 원인이 어디에있는지 알아보자. 원인을 찾기 위해서는 USE라는 방법론을 사요하면 좋다.

- U(Utilization): 리소스 사용량
- S(Saturation): 리소스의 포화도
- E(Error): 에러

여기서 리소스는 크게 하드웨어 리소스와 소프트웨어 리소스로 나뉜다.
- 하드웨어 리소스
    - CPU
    - Disk
    - Network
    - Memory
- 소프트웨어 리소스
    - Thread
    - Connection
    - File

문제가 발생하면 E -> U -> S 순서대로 원인을 찾아간다.

1. E: 시스템이나 애플리케이션의 에러 로그를 확인한다.
2. U: 리소스의 사용률을 확인한다.
3. S: 리소스의 포화도를 확인한다.

#### U(Utilization, 사용률)
사용률이란 단위시간동안 평균적으로 자원을 얼마나 사용했는지를 나타낸다.

- CPU와 Memory의 경우에는 `1분 동안의 사용률 100%`와 같이 표현된다.
- Disk IO, Network IO는 `초당 몇바이트를 읽고 쓰는가`로 표현된다.
- 소프트웨어 리소스는 갯수로 셀 수 있다.
    - ex. 최대 쓰레드 X개 중 X개를 사용하고 있다.
#### S(Saturation, 포화도)
사용률 이라는 개념에는 평균의 문제가 존재한다. CPU 사용률이 100%를 찍은적이 있어도 대부분의 시간동안 사용률이 낮았다면 평균적인 사용률은 낮아지기 때문이다. 그래서 사용률에 문제가 있어보인다면 포화도라는 지표를 관찰하게 된다. 포화도란 각각의 리소스가 포화되었을 때 어떤현상이 일어나는지 관찰함으로써 측정할 수 있다.

CPU, DISK의 경우에는 Run Queue, Device Queue에 대기하고 있는 스레드의 수, 메모리는 Swap 발생 여부, 네트워크는 Overrun, Drop 현상 발생 여부로 알 수 있다.

#### E(Error, 에러)
- 시스템 로그를 기록하는 `/var/log/syslog`, `/var/log/dmesg`에서 `error`, `fail`등의 키워드로 에러를 찾을 수 있다. 
- 애플리케이션은 개발자가 직접 에러로그를 수집할 수 있다. 
- 5XX 상태코드를 가지는 HTTP응답이 얼마나 되는지도 체크해볼 수 있다.

### 원인 찾기
여러 명령어를 사용해서 어떤 프로세스가 하드웨어 리소스를 얼마나 사용하고 있는지 알아낼 수 있다. 스레드 덤프, 힙 덤프, 패킷 덤프를 사용해서 좀 더 자세히 들어가 볼 수 있다.

#### 스레드 덤프 (CPU 이상)
스레드 덤프란 특정 시점의 스레드의 상황에 대해서 기록한 것을 말한다.
- 덤프 시점의 스레드의 상태(RUNNABLE, BLOCKED)
- 호출 스택
- 스레드가 가지고 있는 Lock
등이 그러한 것이다.

리눅스 명령어를 통해서 쉽게 스레드 덤프를 볼 수 있다.
```sh
jps // 현재 실행중인 자바 프로세스의 PID를 얻어온다.
jstack 12275 > thread.dump // 덤프를 가져와서 thread.dump 파일에 쓴다.
```

- 스레드 덤프를 살펴보는 핵심은 멈춰있는 스레드를 찾는 것이다.
- 스레드 덤프를 가져올 때는 5~10초 주기로 최소 5번 가져오는 것이 좋다. ([출처](https://d2.naver.com/helloworld/1286587))
- 스레드가 `RUNNABLE` 상태로 멈춰있는 경우 -> 무한루프에 빠진 경우
- 스레드가 `BLOCKED` 상태로 멈춰있는 경우 -> 데드락이나, 하나의 스레드가 Lock을 오래잡고 있는 상황
- **스레드 덤프는 성능에 영향을 주지않는다.** 따라서 문제가 발생한 경우에는 바로 스레드 덤프를 떠보는 것이 좋다. 

> IntelliJ Ultimate 나 Arthas라는 툴을 사용하면 좀더 편리하게 분석이 가능하다.
> ![인텔리 제이 스레드 덤프 분석](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/eac306a1-ba03-4447-a00a-2cd465a21241)
{: .prompt-tip }

#### 힙 덤프 (메모리 이상)
힙 덤프란 특정 시점에 각 객체의 메모리 점유 상황을 기록하는 것을 말합니다.

다음의 명령어를 사용해서 힙 덤프를 조회할 수 있다. 
```sh
jmap -dump:[live],format=b, file=<file-path> <java process id>
```

예를 들어
```sh
jmap -dump:live,format=b,file=/home/ubuntu/heap.hprof 11541`
```
옵션의 의미는 

- **`-dump:live`** 살아있는 오브젝트 들만
- **`format=b`** 바이너리 형식으로
- **`file=/home/ubuntu/heap.hprof 16574`** `/home/ubuntu` 폴더에 `heap.hprof` 라는 이름으로 PID가 `16574`인 자바 프로세스에 대한 힙 덤프를 생성하라

라는 의미이다.

Intellij Ultimate를 사용한다면 hprof 확장자로 만든 힙 덤프파일을 분석할 수 있자. 자세한 사항은 [Open an external profiling report](https://www.jetbrains.com/help/idea/2021.2/open-an-external-profiling-report.html) 를 확인하자. 인

![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/0b930d8f-a3e7-47de-b484-1bbf7d8f6e82)

위 이미지가 Intellij Ultimate로 힙 덤프를 분석한 내용이다. 힙 덤프는 가져오는데 시간이 오래걸릴 수 있고, 그 시간동안 서비스가 불가능 하기 때문에 Memory Leak이나 OOM(Out Of Memory)가 발생했을 때만 힙 덤프를 떠봐야한다.

#### 패킷 덤프(네크워크 이상)
- 패킷 덤프를 사용하면 일정시간 오고가는 패킷에 대한 상세한 정보를 알 수 있다.
- 요청 IP, 포트번호로 필터링해서 조회할 수도 있다.
- 평소의 패킷 흐름과 문제가 발생했을 때 패킷흐름을 비교해보면서 이상현상을 찾을 수 있다.

다음과 같은 리눅스 명령어로 패킷 덤프를 떠볼 수 있다.
```sh
tcpdump -i eth0 tcp port 8080 -w dump.pcap
```
eth0 네트워크 인터페이스, TCP 프로토콜, 8080 포트를 사용한 패킷 캡쳐본을 dump.pcap 파일로 생성한다.

> 내 네트워크 인터페이스 정보는 `ifconfig` 명령으로 확인할 수 있다.
{: .prompt-tip }

이렇게 생성된 패킷 덤프는 로컬 컴퓨터로 가져와서 Wireshark를 사용해 분석할 수 있다.
![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/2bd9ce7d-fea8-41e7-9514-d8ba9ef432c4)

#### 이러한 것들을 이용해서...
스레드 덤프, 힙 덤프, 패킷 덤프와 같은 여러가지 데이터를 통해서 이상현상과 관련있어보이는 여러가지 단서를 찾을 수 있다. 단서들을 찾았다면 가설을 세우고 검증하는 과정을 통해서 문제의 원인을 파악하게 된다.

지금까지의 방법론을 정의하면 다음과 같다.

1. 문제를 정의하고, 정말로 문제가 맞는지 고민한다.
2. USE 방법론을 사용해 큰 틀에서 원인이 있을만한 곳을 고민한다.
3. 각종 툴을 사용해 세세하게 단서를 수집한다.
4. 가설을 세우고 검증하는 과정을 통해 원인을 파악한다.
5. 왜 문제가 발생하였는지 근본적인 원인을 파악한다.

## 애플리케이션 응답이 없을 때 포인트
### 무한루프
애플리케이션 로직이 무한루프에 빠지게 되면 쓰레드가 CPU하나를 점유하게 되면서 여유 CPU 리소스의 부족으로 처리량이 떨어지고 응답도 느려지게 된다. 문제가 된 무한루프 로직에 다른 CPU자원이 계속해서 들어올 경우에는 무한루프 로직에 CPU자원이 모두 점유당해 응답을 할 수 없게되는 문제로 발전할 수 있다.

이러한 문제가 발생할 경우 CPU사용량이 점차적으로 올라가고, 내려오지 않는다. CPU 코어 하나만 100% 사용되고 있는 것도 무한루프 문제일 가능성이 크다. USE 방법론을 사용해서 CPU 점유율에 문제가 있다는 것을 파악했다면 스레드 덤프를 사용해서 어떤 스레드가 무한루프에 빠진것인지 분석할 수 있다.

### 데드락
데드락이란 락을 잡고있는 스레드들이 서로의 락을 기다리는 상황을 말한다. 데드락이 발생하면 데드락에 걸려있는 락을 중심으로 스레드들이 BLOCKED 상태로 빠지게 된다. BLOCKED된 스레드들은 스레드 풀로 반환되지 않고 스레드 풀의 스레드 들은 시간이 지날 수록 수가 증가하게 된다. 그러다 스레드 풀이 허용할 수 있는 스레드의 수를 넘어서게 되면 더이상 요청에 응답할 수 없게 된다.

데드락이 발생하게 되면 스레드들이 BLOCKED된 상태이기 때문에 CPU사용량은 낮지만 요청에 응답할 수 없는 상태가 된다. 스레드덤프를 이용해서 데드락이 발생했는지 알아낼 수 있다. 스레드 덤프를 분석하면 데드락이 있을 경우 데드락이 있다고 표시된다.

### 메모리 릭
메모리 릭이란 사용하지 않는 객체들로 메모리가 가득차 부족해지는 현상을 말합니다.

메모리 릭이 발생하면 사용가능한 메모리의 영역이 점차 줄어들면서 잦은 Full GC가 일어나 애플리케이션이 자주 멈추고 느려지는 현상이 발생합니다. 메모리 릭으로 인해 메모리가 가득차면 GC관련 스레드만 실행되면서 CPU 코어 하나를 100% 점유하게 되고, 다른 스레드들은 아무런 작업도 하지 못하게 됩니다. 그러다 GC조차도 사용할 수 있는 메모리가 없어지면 OOM(Out Of Memory)Error 가 발생하여 애플리케이션이 다운될 수 있습니다.

메모리 릭은 일반적으로 Full GC가 자주 발생하면서 메모리 사용률이 80~90%를 유지할 때 의심해볼 수 있습니다. 또 메모리 릭이 발생하게 되면 힙 덤프를 이용해서 메모리를 많이 잡고있는 객체를 파악할 수 있습니다.

> `OOM`이 발생하면 힙 덤프를 뜨지 못한다. JVM의 시작 옵션으로 `-XX:+HeapDumpOnOutOfMemoryError` 를 주면 OOM발생 시 자동으로 힙덤프를 떠준다.
{: .prompt-tip }

### 설정문제
- 스레드 풀의 사이즈
- DB 커넥션 풀 사이즈
- DB 커넥션 타임아웃

등의 여러 설정정보를 조정해서 문제를 해결할 수도 있습니다. 이 부분은 성능 테스트를 진행하며 최적의 값을 찾아 나가야 합니다.

### 디스크 용량 포화
- 디스크 용량이 가득차면 시스템이 아무런 출력을 하지 못해서 멈추어 버리는 `hang` 현상이 발생할 수 있다.
- 보통 로그파일로 인해 디스크의 용량이 가득 차는 경우가 많다.
- 주기적으로 백업, 압축, 이동등을 통해 로그파일을 관리해주는 것이 좋다. 
    - 애플리케이션의 설정으로 특정 용량이상이 되면 로그파일을 분리하거나 삭제하는 방법도 있다.

## 원인 파악하기
1. Vuser 10명부터 아이템 조회 응답 속도를 달성하지 못함
2. Vuser 25명부터 아이템 조회 응답시간 증가
3. Vuser가 40명이 넘어가면 프로세스 CPU 사용률 100%

- 두 테스트 모두 Vuser가 10쯤 되는 시점에서 포화점이 시작되고 25명쯤일 때 경합구간에 진입한다.
- 경합구간에 들어간 후 응답시간도 160ms 밑으로 내려오지 않아 목표인 150ms에 미치지 못한다.
- 경합구간에 들어간 후 CPU사용량은 최대 70%로 꽤있는 편이지만 아직 사용할 수 있는 리소스가 남아있는 상태.
- WAS의 스레드 풀에서 사용할 수 있는 스레드의 수도 여유가 있음.

## Reference
- [`Whatap` 리눅스 네트워크 관리에 필요한
`ifconfig` 사용법 11가지](https://www.whatap.io/ko/blog/11/)
- [`Velog` 트러블 슈팅, 문제의 원인을 찾아가는 과정](https://velog.io/@sontulip/how-to-shoot-trouble)
- [`Naver D2` 스레드덤프 분석기법과 사례
](https://d2.naver.com/helloworld/1286587)
- [`Naver D2` 하나의 메모리 누수를 잡기까지](https://d2.naver.com/helloworld/1326256)