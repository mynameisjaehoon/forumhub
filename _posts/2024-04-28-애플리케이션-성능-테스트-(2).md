---
title: 애플리케이션 성능 테스트 (2)
date: 2024-04-28 16:00:00 +09:00
description: >-
    애플리케이션의 성능 테스트 결과를 분석하고 문제를 해결해나가는 과정에 대해서 설명합니다.
categories: [성능 테스트]
tags: [메이플 주문서 시뮬레이터]
---

## 튜닝 목표
문제를 해결하기 이전에 목표를 다시 한번 살펴보자.<br>
사용자가 최초 홈페이지에 접근하면 서버에 다음과 같은 목록을 요청한다.

1. 아이템 목록
2. 인기 아이템 랭킹

RPS가 480인 상황에서 `아이템 목록`과 `인기 아이템 랭킹 목록`을 150ms 내에 응답해주는 것이 목표이다.

## 테스트 진행
### 테스트 내용
1시간 동안 70초에 한번씩 `Vuser`의 수를 하나씩 늘려가며 1시간동안 부하테스트를 진행했습니다. 최대 `Vuser`의 수는 50입니다.
### 테스트 결과
![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/c0a022e6-ad67-4046-9429-83f50d77818e)
![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/5bd86d92-289e-4df6-9d56-db617efb9c9b)

- 프로세스 CPU 사용률이 100%에 근접했습니다. 이 부분은 하드웨어적인 문제이기 때문에 
  - VM 인스턴스의 스펙을 더 좋은 것으로 바꾸거나 
  - WAS를 여러 개 구축하고 로드 밸런서로 트래픽을 분산하는 방법으로 해결할 수 있습니다.
- Vuser가 10명이 되는 지점에서 TPS가 급감하고, 응답시간은 급증하는 모습을 보입니다.
- Vuser가 27명이 되는 지점에서 사용되는 DB 커넥션의 수가 급증하였다. 이후 커넥션 갯수가 부족해 커넥션을 할당받기를 기다리는 `Pending` 현상이 발생하였습니다. 애플리케이션에 무리되지 않는 선에서 데이터베이스 커넥션 풀의 커넥션 수를 증가시키는 방법으로 문제를 해결할 수 있습니다.

![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/d105baf1-1df5-484d-82d7-416acffe257a)
![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/d1b4bc5c-69d5-4119-a9a7-5e882c62e7aa)
![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/4d5bfb38-3407-470d-9a50-7c58d9e8c7c0)
![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/6379d568-5bb8-4d75-8758-7d326029cf09)

위 이미지는 커넥션의 갯수가 부족해지는 병목지점을 콜스택으로 살펴본 것입니다.
`@Transactional`애노테이션을 사용했기 때문에 트랜잭션을 시작하기 위해서 커넥션을 얻어와야 합니다. 여유 커넥션이 부족하기 때문에 커넥션 풀에서 커넥션을 얻어오는 `getConnection()` 메서드에서 병목이 발생하는 것을 확인할 수 있습니다.

## DBCP 커넥션 갯수 조정
커넥션을 얻지 못하고 여유 커넥션이 생길 때까지 대기하는 작업때문에 응답속도가 느려지는 현상이 관측했습니다
커넥션 풀의 커넥션 갯수를 현재의 2배인 `20개`로 늘리고 다시 한번 테스트를 해보겠습니다.

`Spring Boot 2.0` 부터는 커넥션 풀 라이브러리로 `HikariCP`를 사용하고 있습니다. `HikariCP` 설정 정보를 변경해 커넥션의 갯수를 변경해봅시다.

```yml
# application.yml
spring:
  datasource:
    hikari:
      maximum-pool-size: 20
      minimum-idle: 20
```
커넥션 풀의 커넥션 최대 갯수를 기본값인 `10`에서 `20`으로 수정하였습니다.

![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/a22459f7-4e49-4683-9528-403d4a84f17e)

다시 테스트를 수행한 결과 대기하는 커넥션은 없어졌지만 여전히 응답에는 지연이 발생했습니다. CPU 사용률이 계속해서 100%를 유지하면서 CPU의 처리능력 한계로 대기시간이 증가했기 때문입니다.

## Scale Out
CPU 사용률이 100%에 도달하면 CPU가 추가적인 작업을 처리할 여유가 없기 때문에 도착한 요청들을 큐에 담아두고 대기시킵니다. 새로운 요청이 즉시 처리되지 않고 대기해야하기 때문에 그만큼 응답시간도 느려지는 것입니다. 이를 해결하기 위해서는 작업 부하를 줄이거나 수평적 확장(Scale Out)을 통해 더 많은 CPU 자원을 확보해야합니다. 저는 우선적으로 Scale Out을 통해서 문제를 해결해보고자 하였습니다. 작은 인스턴스를 몇개 더 만드는 것이 비용적으로 부담이 없고, 이후에 무중단 배포 작업을 위한 초석으로도 사용할 수 있기 때문입니다.

WAS로 사용할 서버 인스턴스 2개와 WAS로 트래픽을 분산할 로드밸런서 인스턴스를 새롭게 만들었습니다. 로드밸런서는 Nginx를 사용하여 구현하였습니다. 자세한 내용은 [Nginx를 위한 로드밸런싱](https://mynameisjaehoon.github.io/posts/Nginx%EB%A5%BC-%EC%9C%84%ED%95%9C-%EB%A1%9C%EB%93%9C%EB%B0%B8%EB%9F%B0%EC%8B%B1/)을 참고해주세요

로드밸런서를 활용한 Scale Out으로 서버의 CPU포화도가 증가해 처리량이 급격하게 감소하는 현상이 해결되었습니다.

![](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/1945833c-7b96-4f2a-9d2a-cb8eff035b34)

또 두 인스턴스의 CPU 사용량도 많이 줄어든 것을 확인할 수 있습니다. `100%`에 여러번 도달했던 이전과는 다르게 높아야 `30%`의 사용률을 보입니다.
![Scale Out 후 CPU 사용량](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/f7404c2a-b65e-4056-9fd8-005ae5cbd119)

응답시간 점도표를 살펴보아도 Vuser가 30명이 넘어가면 2000~3000ms대의 응답시간을 보였던 것과도 확연한 차이를 보입니다. 

![Pinpoint_Scatter_Chart](https://github.com/mynameisjaehoon/mynameisjaehoon.github.io/assets/76734067/395e2059-9383-4026-8828-5384a4840e95)

성능측면에서 Scale Out 이전과 비교해보면

|목록|단일 서버|Scale Out(서버 2개)|
| :----------- | :-------- | :------------------- |
| **한계점**      | Vuser 8명 | Vuser 12명 |
| **경합지점**     | Vuser 10명 | Vuser 30명 |
| **평균 응답시간** | 333ms | 185ms |
| **최대 응답시간** | 3200ms | 209ms |
| **평균 TPS**   | 120 | 100 + 100 = 200 |

- `평균 응답시간`에서도 2배 정도의 성능 향상을, `최대 응답시간`관점에서는 극단적으로 응답시간이 느려지는 현상이 사라졌습니다.
- 서버 성능의 척도를 나타내는 `TPS`도 2배가량 성능 향상이 이루어졌습니다.

### 한계점
위의 결과에서도 알 수 있듯이 한계점에 도달한 시점에도 CPU의 사용량은 20~30%에 불과한 것을 볼 수 있습니다. 이는 성능에 안좋은 영향을 미치는 다른 요인이 있다는 것을 의미합니다.

## 캐시 추가
커넥션이 필요한 이유는 DB에 요청 쿼리를 보내고 응답 데이터를 받기 위함입니다. 그런데 만약 자주 조회되는 데이터가 캐시에 저장되어 있다면 DB 요청을 할 필요도 없어지기 때문에 커넥션을 얻어올 필요도 없어지고 그만큼 응답 시간을 줄일 수 있습니다.

### 로컬 VS 리모트
캐시는 크게 `로컬 캐시`와 `리모트 캐시`로 나눌 수 있습니다. 로컬 캐시는 서버내 메모리를 캐시로 활용하는 방법이고, 리모트 캐시는 WAS 서버가 아닌 다른 서버에 캐시 저장소를 두고 필요한 데이터를 활용하는 방법입니다.

둘 중에 무엇을 선택할지는 캐싱 대상 사이즈와, 업데이트 주기로 결정됩니다.

로컬 캐시의 경우에는 WAS의 메모리에 저장하는 것이기 때문에 따로 캐시 서버를 두는 것보다 여유공간이 적습니다. 캐시의 사이즈가 너무 크면 JVM 애플리케이션의 경우 GC의 빈도가 늘어나 성능에 안좋은 영향을 줄 수 있습니다. 또 여러 대의 WAS로 운영되는 상황에서는 서로 간의 캐시 동기화를 이뤄내기 위한 작업이 필요합니다. 예를 들어서 한 서버에서 캐시 업데이트가 일어날 때 다른 서버에 업데이트 사실을 전달하는 방식으로 운영된다고 가정해보겠습니다. 그러면 캐시 업데이트가 일어날 때마다 네트워크를 사용하게 되고 WAS가 늘어날 수록 이 비용은 증가하게 됩니다. 반대로 업데이트 사실을 전달하지 않는다고 가정해보면 각 WAS 서버간에 캐시 정보의 불일치가 일어나는 것을 피할 수 없게됩니다.

리모트 캐싱은 이러한 문제가 없습니다. 하지만 캐시서버가 다운될 때를 대비해서 이중화 구성을 하는 등 시스템의 복잡도가 늘어나게되고 캐싱 저장소를 유지하는데 필요한 추가적인 비용이 듭니다. 그리고 매번 캐시 저장소에 네트워크를 타고 접근해야하기 때문에 로컬 캐싱에 비해 성능이 떨어집니다.

